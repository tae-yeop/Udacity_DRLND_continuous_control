{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Run the next code cell to install a few packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Learning Algorithm\n",
    "\n",
    "---\n",
    "\n",
    "To solve Unity reacher one agent problem, I choose [DDPG algorithm](https://arxiv.org/pdf/1509.02971.pdf)(Lillicrap et al., CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING).\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "To make DQN agent deal with continuos action space, the authors suggest extension of actor-critic model called DDPG. The DDPG agent is consists of 2 kinds of networks. Actor network is responsible for the policy function approximator. Critic Network is reponsible for Q-value function approximator. \n",
    "\n",
    "Just like DQN, DDPG also uses replay buffer which stores old experiences and samples a small batch of tuples to remove correlations in consecutive observations. They also use target network used in DQN, but modified it to use soft target updates.\n",
    "\n",
    "And they add Ornstein-Uhlenbeck Noise to the action produced by actor network for encouraging exploration. Lastly, they used Adam optimizer for learning the nueral network paramters.\n",
    "\n",
    "<br>\n",
    "<figure>\n",
    "  <img src = \"./ddpg_algorithm.png\" width = 80% style = \"border: thin silver solid; padding: 10px\">\n",
    "      <figcaption style = \"text-align: center; font-style: italic\">Fig 1. - DDPG Algorithm.</figcaption>\n",
    "</figure> \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparamters**\n",
    "\n",
    "All these hyperparamters except buffer size and batch size are from the paper's experiment details. Buffer size and batch size are much smaller since the task is more simple.\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 1e-2     # L2 weight decay\n",
    "SIGMA = 0.2             # Paramter for OU Process\n",
    "THETA = 0.15            # Paramter for OU Process\n",
    "\n",
    "```\n",
    "\n",
    "**Model Architecture**\n",
    "\n",
    "Since this reacher problem is low dimensional problem, Both Actor and Critic are consist of few several fully connected layers. Critic gets states and actions as input and ouputs the action-value. The paper states that the actions were not included until the 2nd hidden layer of Q. So In this Implementatiom, actions are merged into the hidden layer between the 1st and 2nd one.\n",
    "\n",
    "```\n",
    "Actor(\n",
    "  (fc1): Linear(in_features=33, out_features=64, bias=True)\n",
    "  (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
    "  (fc3): Linear(in_features=128, out_features=4, bias=True)\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "Critic(\n",
    "  (fc1): Linear(in_features=33, out_features=64, bias=True)\n",
    "  (fc_merged): Linear(in_features=68, out_features=128, bias=True)\n",
    "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
    "  (fc3): Linear(in_features=64, out_features=4, bias=True)\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Implementation\n",
    "\n",
    "\n",
    "- Replay Buffer\n",
    "- Actor and Critic Network\n",
    "- OUNoise\n",
    "- Agent\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as I\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [True, True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.mse_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([3.0], requires_grad=False)\n",
    "print(a.view(-1).requires_grad)\n",
    "with torch.no_grad():\n",
    "    b = a**2\n",
    "    c = b*2\n",
    "    d = c*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(b.requires_grad)\n",
    "print(c.requires_grad)\n",
    "print(d.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([3.0, 2.0], requires_grad=True)\n",
    "b = torch.tensor([4.0, 1.0], requires_grad=False)\n",
    "hubber_loss = torch.nn.SmoothL1Loss()\n",
    "critic_loss = hubber_loss(a, target=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5000,  0.5000])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.smooth_l1_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[3.0], [2.0]], requires_grad=True)\n",
    "b = torch.tensor([[4.0], [1.0]], requires_grad=False)\n",
    "loss = F.mse_loss(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.],\n",
      "        [ 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Variable(torch.tensor([3.0]), requires_grad=True)\n",
    "b = a**2\n",
    "c = b*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 36.])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "d = c*3\n",
    "d.backward(retain_graph=True)\n",
    "\n",
    "print(a.grad)\n",
    "print(d.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 36.])\n"
     ]
    }
   ],
   "source": [
    "d = c*3\n",
    "a.grad.zero_()\n",
    "d.backward(retain_graph=True)\n",
    "\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 36.])\n"
     ]
    }
   ],
   "source": [
    "d = c*3\n",
    "a.grad.zero_()\n",
    "d.backward(retain_graph=True)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1647, -7.3983, -3.2755,  7.9343]])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(data=[1], requires_grad=True)\n",
    "y =x**2\n",
    "z = 2*y\n",
    "w = z**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(y.requires_grad)\n",
    "print(z.requires_grad)\n",
    "print(w.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.tensor(data=[2], requires_grad=True)\n",
    "pq = p*q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 56])\n"
     ]
    }
   ],
   "source": [
    "w.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1], requires_grad=True)\n",
    "y = x**2\n",
    "z = 2*y\n",
    "w = z**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = z.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.tensor([2], requires_grad=True)\n",
    "pq = p*q\n",
    "pq.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 48])\n"
     ]
    }
   ],
   "source": [
    "w.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "drop = nn.Dropout(0.5)\n",
    "x = torch.ones(1,10)\n",
    "\n",
    "drop.train()\n",
    "print(drop(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "drop.eval()\n",
    "print(drop(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a=torch.randn(2,4)\n",
    "print(a.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "b=a.permute(1,0)\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.randn(2,4)\n",
    "b = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8118, -0.8935,  0.5180,  0.4833])\n",
      "1\n",
      "tensor([ 0.7906, -0.9023,  2.2028, -0.8669])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for t,t2 in zip(a,b):\n",
    "    print(t)\n",
    "    print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8118, -0.8935,  0.5180,  0.4833],\n",
       "         [ 0.7906, -0.9023,  2.2028, -0.8669]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.random.randn(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.71846398, -0.803826  ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 30])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(20, 30)\n",
    "mb = nn.BatchNorm1d(30)\n",
    "input = torch.randn(40,20)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 30])\n"
     ]
    }
   ],
   "source": [
    "mb.eval()\n",
    "output = mb(output)\n",
    "mb.train()\n",
    "print(output.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in [1,2,3,4,5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [np.random.randn(2), np.random.randn(2), np.random.randn(2), np.random.randn(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.13954628,  1.18039878]),\n",
       " array([-1.4491894 ,  0.04311405]),\n",
       " array([ 0.90179285, -1.69994976]),\n",
       " array([0.91887564, 0.06323698])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.25208563, -0.66612742, -1.51034224, -0.27338439])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(4,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52165329],\n",
       "       [-0.86011339],\n",
       "       [-0.44075813],\n",
       "       [ 0.54924978]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(1,4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.15024536, -1.12597114,  1.55068629, -0.11011235]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.48978565,  1.35344593, -0.53593591, -0.07508489]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(4)+np.random.randn(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(4).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Replay Buffer\n",
    "\n",
    "Reinforcement learning is unstable when a nonlinear function approximator is used to represent action-value function, because the sequence of experiencs can be highly correlated. DDPG Agent stores the experience at each time step. Then by sampling from the buffer at random, It can prevent action values from oscillating or diverging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import random\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, buf_size, batch_size, seed):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ----------\n",
    "        buf_size (int): size of memory\n",
    "        batch_size (int): number of samples to be sampled\n",
    "        seed (int): random seed\n",
    "        \"\"\"\n",
    "        # When the replay buffer was full, the oldest sample needs to be discarded.\n",
    "        # So deque is suitalbe data structure. \n",
    "        self.memory = deque(maxlen=buf_size)\n",
    "        self.seed = random.seed(seed)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple('Trajectory', field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the size of memory\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(self.memory)\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add the agent's experiences at eacy time to the memory\n",
    "        \"\"\"\n",
    "        # Instantiate new experience with custom nemaedTuple\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        # Add the tuple to the memory\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Draw a sample.\n",
    "        Since the sample data is used by pytorch model, It needs to be converted to a torch Tensor.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        A tuple of torch tensor. Each tenosr's outermost dimension is batch_size.\n",
    "        \"\"\"\n",
    "        # list of sampled experience namedtuple of size of self.batch_size\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        # states : [batch_size, state_size]\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        # dones is needed to calculated the Q-value. At terminal state(dones=1), the Q-value should be just latest rewards.\n",
    "        # Convert it to np.uint8\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Actor and Critic Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the papar, The authors initialized the final layer weights and biases of both the actor and critic from a uniform distribution $ [−3×10^−3, 3×10^−3] $ and $[3×10^−4, 3×10^−4]$. This was to ensure the initial outputs for the policy and value estimates were near zero. The other layers were initialized from uniform distributions $ [− \\sqrt{f} , \\sqrt{f} ] $ where f is the fan-in of the layer.\n",
    "\n",
    "Since eveny entry in the action vector should be a number between -1 and 1, The activation function is **tanh**.\n",
    "Other hidden layers is activated by **relu**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy Network : state -> specific action (Not Probability distribution of Actions)\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128):\n",
    "        \"\"\"\n",
    "        Initialization\n",
    "        \n",
    "        Params\n",
    "        -------\n",
    "        state_size : Vector Observation space size(per agent)\n",
    "        action_size : Vector Action space size(per agent) \n",
    "        seed : seed\n",
    "        fc1_units : first hidden layer \n",
    "        fc2_units : seccond hidden layer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        actions (Torch Tensor)\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        #self.seed = torch.manual_seed(seed)\n",
    "        self.bn0 = nn.BatchNorm1d(state_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        \n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.bn2 = nn.BatchNorm1d(fc2_units)\n",
    "        \n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def get_fan_in(self, layer):\n",
    "        \"\"\"\n",
    "        Get the fan-in in each layer.\n",
    "        \"\"\"\n",
    "        fan_in = 1/np.sqrt(layer.in_features)\n",
    "        return -fan_in, fan_in\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize weights and bais in each layer\n",
    "        \"\"\"\n",
    "        I.uniform_(self.fc1.weight, *self.get_fan_in(self.fc1))\n",
    "        I.uniform_(self.fc2.weight, *self.get_fan_in(self.fc2))\n",
    "        I.uniform_(self.fc3.weight, -3*1e-3, 3*1e-3)\n",
    "            \n",
    "    def forward(self, state):\n",
    "        \n",
    "        x = self.bn0(state)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        # Every entry in action vector : [-1, 1]\n",
    "        actions = F.tanh(self.fc3(x))\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128):\n",
    "        super(Critic, self).__init__()\n",
    "        #self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        # hidden layer for state pathway\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        \n",
    "        self.fc_merged = nn.Linear(fc1_units+action_size, fc2_units)\n",
    "        self.bn2 = nn.BatchNorm1d(fc2_units)\n",
    "        \n",
    "        self.fc2 = nn.Linear(fc2_units, 1)\n",
    "\n",
    "        \n",
    "        # Initialize Weights and Biases\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        I.uniform_(self.fc1.weight, *self.get_fan_in(self.fc1))\n",
    "        I.uniform_(self.fc_merged.weight, *self.get_fan_in(self.fc_merged))\n",
    "        I.uniform_(self.fc2.weight, -3*1e-4, 3*1e-4)\n",
    "        #I.uniform_(self.fc3.weight, -3*1e-3, 3*1e+3)\n",
    "        \n",
    "    def get_fan_in(self, layer):\n",
    "        fan_in = 1/np.sqrt(layer.in_features)\n",
    "        return -fan_in, fan_in\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        \n",
    "        x = F.relu((self.fc1(state)))\n",
    "        x = self.bn1(x)\n",
    "                   \n",
    "        # state : [batch_size, state_size], action : [batch_size, action_size]\n",
    "        # merged : [batch_size, state_size + action_size]\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        #x = self.bn2(x)\n",
    "        \n",
    "        x = F.relu(self.fc_merged(x))\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        #x = F.relu(self.bn3(self.fc2(x)))\n",
    "        q = self.fc2(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.OUNoise\n",
    "\n",
    "To encourage agnet do exploration at initial step, add noise from Ornstein–Uhlenbeck noise process to the specific action produced by the actor(policy) network. The variation of the noise process decreases as time goes by. Therefore it can lead to reducing the exploration as the agent train. Also 2 consecutive samples are temporally correlated. This will ensure that 2 consecutive actions are not different widly. The authors use theta=0.15 sigma=0.2 for this noise process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"\n",
    "    Ornstein–Uhlenbeck noise\n",
    "    \"\"\"\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.1):\n",
    "        \"\"\"\n",
    "        Intialization\n",
    "        \n",
    "        Params\n",
    "        ------\n",
    "        size : number of noise. It should be action_size = size\n",
    "        seed : random seed\n",
    "        mu : mean, defalut : 0 to reduce the noise over time.\n",
    "        theta : hyper paramter\n",
    "        sigam : hyper paramter\n",
    "        \"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the internal state to mu\n",
    "        \"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Process the internal state.\n",
    "        The Wiener process states are sampled by random.random()\n",
    "        \n",
    "        Returns\n",
    "        -----\n",
    "        self.state (numpy array)\n",
    "        \"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma*np.random.randn(len(x))\n",
    "        self.state = x+dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 256        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4        # learning rate of the actor \n",
    "LR_CRITIC = 1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 1e-2     # L2 weight decay for the critic\n",
    "UP_FREQ = 4\n",
    "class DDPGAgent():\n",
    "    \"\"\"\n",
    "    RL Agent whose actor and critic networks \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"\n",
    "        Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        -------\n",
    "        state_size : \n",
    "        \n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Actor networks\n",
    "        self.actor_local = Actor(state_size, action_size, seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        # Critic networks\n",
    "        self.critic_local = Critic(state_size, action_size, seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        # Replay buffer\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # OUNoise\n",
    "        self.noise = OUNoise(size=action_size, seed=self.seed)\n",
    "        \n",
    "        \n",
    "        self.actor_loss = []\n",
    "        self.critic_loss = []\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the OUNoise state.\n",
    "        \"\"\"\n",
    "        self.noise.reset()\n",
    "        \n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"\n",
    "        Returns actions for state based on current policy.\n",
    "        Params\n",
    "        -----\n",
    "        state : 1d numpy array [state_size]\n",
    "        add_noise : boolean , default: True\n",
    "        Returns\n",
    "        -----\n",
    "        action : numpy array ,shape:[1 x action_size]\n",
    "        \"\"\"\n",
    "        # Convert state into torch tensor \n",
    "        # Use unsqueeze(0) for batch normalization. [1 x state_size]\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        # Set evaluation mode\n",
    "        self.actor_local.eval()\n",
    "        # Disable gradient calculation\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy() # [1 x action_size]\n",
    "        self.actor_local.train()\n",
    "        \n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "            \n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store experience in replay buffer\n",
    "        \"\"\"\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.memory)>BATCH_SIZE:\n",
    "            # Get minibatch experiences\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"\n",
    "        Update actor and critic network\n",
    "        \n",
    "        Params\n",
    "        -----\n",
    "        experiences (Tuple[torch.Tensor])\n",
    "        gamma (float)\n",
    "        \"\"\"\n",
    "        # Each of them is batch sized torch Tenosr\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # Update local critic\n",
    "        # 1. Get next actions\n",
    "        next_actions = self.actor_target(next_states)\n",
    "        # 2. Get target value from the target network \n",
    "        q_next = self.critic_target(next_states, next_actions)\n",
    "        y = rewards + gamma*q_next*(1-dones)\n",
    "        # 3. Critic objective function\n",
    "        critic_loss = F.mse_loss(self.critic_local(states, actions), y)\n",
    "        # 4. Minimize\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        nn.utils.clip_grad_value_(self.critic_local.parameters(),1.0)\n",
    "        self.critic_optimizer.step()\n",
    "        self.critic_loss.append(critic_loss.cpu().data.numpy())\n",
    "        \n",
    "        # Update local actor\n",
    "        # 1. Get predicted action\n",
    "        pred_actions = self.actor_local(states)\n",
    "        # 2. Actor objective\n",
    "        actor_loss = -self.critic_local(states, pred_actions).mean()\n",
    "        # 3. Minimize loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        # nn.utils.clip_grad_value_(self.actor_local.parameters(),1.0)\n",
    "        self.actor_optimizer.step()\n",
    "        self.actor_loss.append(actor_loss.cpu().data.numpy())\n",
    "        # Update target networks\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        \n",
    "        \n",
    "    def soft_update(self, local, target, tau):\n",
    "        \"\"\"\n",
    "        Soft update\n",
    "        \n",
    "        𝜃_target = 𝜏*𝜃_local + (1 - 𝜏)*𝜃_target\n",
    "        \n",
    "        \"\"\"\n",
    "        # .parameters() is generator\n",
    "        for target_param, local_param in zip(target.parameters(), local.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1-tau)*local_param.data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgent():\n",
    "    \n",
    "    def __init__(self, n_agents, state_size, action_size, seed):\n",
    "        \n",
    "        self.shared_buffer = ReplayBuffer(BUFFER_SIZE,BATCH_SIZE,seed)\n",
    "        self.n_agents = n_agents\n",
    "        self.add_noise = False\n",
    "        self.agent_list = [DDPGAgent(state_size, action_size, seed) for i in range(n_agents)]\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        for i in range(self.n_agents):\n",
    "            self.agent_list[i].reset()\n",
    "            \n",
    "    def act(self, states):\n",
    "        act_list = []\n",
    "        for i in range(self.n_agents):\n",
    "            act_list.append(self.agent_list[i].act(states[i], self.add_noise))\n",
    "        return act_list\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"\n",
    "        Store experience in replay buffer\n",
    "        \"\"\"\n",
    "        for i in range(self.n_agents):\n",
    "            self.shared_buffer.add(states[i], actions[i], rewards[i], next_states[i], dones[i])\n",
    "        #self.t_step = (self.t_step+1) % UP_FREQ\n",
    "        #if self.t_step == 0:\n",
    "        #for i in range(UP_FREQ):\n",
    "        if len(self.shared_buffer)>BATCH_SIZE:\n",
    "            # Get minibatch experiences\n",
    "            for j in range(self.n_agents):\n",
    "                experiences = self.shared_buffer.sample()\n",
    "                self.agent_list[j].learn(experiences, GAMMA)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Plot of Rewards\n",
    "---\n",
    "### 1.Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "# for linux : /data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64\n",
    "env = UnityEnvironment(file_name='data/single_agent/Reacher_Windows_x86_64/Reacher.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n",
      "Shape of next state: (1, 33)\n",
      "Shape of the rewards : [0.0]\n",
      "Shape of dones : [False]\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "\n",
    "# shape of its elements\n",
    "print('Shape of next state: {}'.format(env_info.vector_observations.shape))\n",
    "print('Shape of the rewards : {}'.format(env_info.rewards))\n",
    "print('Shape of dones : {}'.format(env_info.local_done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 33)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33,)\n"
     ]
    }
   ],
   "source": [
    "for state in states:\n",
    "    print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.09549999786540866\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPGAgent(state_size=state_size, action_size=action_size, seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(\n",
       "  (bn0): BatchNorm1d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=33, out_features=256, bias=True)\n",
       "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.actor_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Critic(\n",
       "  (fc1): Linear(in_features=33, out_features=256, bias=True)\n",
       "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc_merged): Linear(in_features=260, out_features=128, bias=True)\n",
       "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.critic_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "a=0\n",
    "\n",
    "if a==0:\n",
    "    print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=200, timestep_max=1000, print_every=50):\n",
    "    # for plotting score grpha\n",
    "    scores = []\n",
    "    # for calculating mean score of consecutive episodes.\n",
    "    score_deque = deque(maxlen=100)\n",
    "    for i_episode in range(1,n_episodes):\n",
    "        # Get Initial State\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        # noise reset\n",
    "        agent.reset()\n",
    "        # Episode score\n",
    "        score = 0\n",
    "        for t in range(timestep_max):\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "        \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        score_deque.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode: {}\\t Score: {:.2f}'.format(i_episode, score), end=\"\")\n",
    "        # save model parameters\n",
    "        if i_episode%print_every == 0:\n",
    "            print('Episode: {}\\t Average score: {}'.format(i_episode, np.mean(score_deque)))\n",
    "        if np.mean(score_deque)>=30:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break           \n",
    "            \n",
    "    return scores, agent.actor_loss, agent.critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_agent=MultiAgent(20, state_size, action_size, 0)\n",
    "\n",
    "def ddpg_multi(n_episodes=1000, timestep_max=1000, print_every=50):\n",
    "    # for plotting score grpha\n",
    "    scores = []\n",
    "    # for calculating mean score of consecutive episodes.\n",
    "    score_deque = deque(maxlen=100)\n",
    "    for i_episode in range(n_episodes):\n",
    "        # Get Initial State\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        # noise reset\n",
    "        multi_agent.reset()\n",
    "        # Episode score\n",
    "        score = np.zeros(20)\n",
    "        for t in range(timestep_max):\n",
    "            actions = multi_agent.act(states)\n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "        \n",
    "            multi_agent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            score += rewards\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        avg_score = score.mean()\n",
    "        score_deque.append(avg_score)\n",
    "        scores.append(avg_score)\n",
    "        print('\\rEpisode: {}\\t score: {}'.format(i_episode, avg_score), end=\"\")\n",
    "        # save model parameters\n",
    "        if i_episode%print_every == 0:\n",
    "            print('Episode: {}\\t Average score: {}'.format(i_episode, np.mean(score_deque)))\n",
    "#         if np.mean(score_deque)>=30:\n",
    "#             torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "#             torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "#             break           \n",
    "            \n",
    "    return scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\t score: 0.012999999709427357Episode: 0\t Average score: 0.012999999709427357\n",
      "Episode: 13\t score: 0.026999999396502973"
     ]
    }
   ],
   "source": [
    "scores= ddpg_multi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50\t Score: 0.15Episode: 50\t Average score: 0.5887999868392945\n",
      "Episode: 100\t Score: 0.24Episode: 100\t Average score: 0.636699985768646\n",
      "Episode: 108\t Score: 0.33"
     ]
    }
   ],
   "source": [
    "scores, actor_loss, critic_loss= ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(actor_loss+1), 1), actor_loss)\n",
    "plt.ylabel('average score')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(critic_loss+1), 1), critic_loss)\n",
    "plt.ylabel('average score')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hyper paramter를 조정해본다\n",
    "    \n",
    "    Authors get the optiaml hyper paratmeter by extensive trial to the task. and obviously the task in the paper and this project task are different. so there might be a better set of hyperparamters.\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
