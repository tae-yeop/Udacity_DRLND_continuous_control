{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Run the next code cell to install a few packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Learning Algorithm\n",
    "\n",
    "---\n",
    "\n",
    "To solve Unity reacher one agent problem, I choose [DDPG algorithm](https://arxiv.org/pdf/1509.02971.pdf)(Lillicrap et al., CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING).\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "To make DQN agent deal with continuos action space, the authors suggest extension of actor-critic model called DDPG. The DDPG agent is consists of 2 kinds of networks. Actor network is responsible for the policy function approximator. Critic Network is reponsible for Q-value function approximator. \n",
    "\n",
    "Just like DQN, DDPG also uses replay buffer which stores old experiences and samples a small batch of tuples to remove correlations in consecutive observations. They also use target network used in DQN, but modified it to use soft target updates.\n",
    "\n",
    "And they add Ornstein-Uhlenbeck Noise to the action produced by actor network for encouraging exploration. Lastly, they used Adam optimizer for learning the nueral network paramters.\n",
    "\n",
    "<br>\n",
    "<figure>\n",
    "  <img src = \"./ddpg_algorithm.png\" width = 80% style = \"border: thin silver solid; padding: 10px\">\n",
    "      <figcaption style = \"text-align: center; font-style: italic\">Fig 1. - DDPG Algorithm.</figcaption>\n",
    "</figure> \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparamters**\n",
    "\n",
    "All these hyperparamters except buffer size and batch size are from the paper's experiment details. Buffer size and batch size are much smaller since the task is more simple.\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 1e-2     # L2 weight decay\n",
    "SIGMA = 0.2             # Paramter for OU Process\n",
    "THETA = 0.15            # Paramter for OU Process\n",
    "\n",
    "```\n",
    "\n",
    "**Model Architecture**\n",
    "\n",
    "Since this reacher problem is low dimensional problem, Both Actor and Critic are consist of few several fully connected layers. Critic gets states and actions as input and ouputs the action-value. The paper states that the actions were not included until the 2nd hidden layer of Q. So In this Implementatiom, actions are merged into the hidden layer between the 1st and 2nd one.\n",
    "\n",
    "```\n",
    "Actor(\n",
    "  (fc1): Linear(in_features=33, out_features=64, bias=True)\n",
    "  (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
    "  (fc3): Linear(in_features=128, out_features=4, bias=True)\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "Critic(\n",
    "  (fc1): Linear(in_features=33, out_features=64, bias=True)\n",
    "  (fc_merged): Linear(in_features=68, out_features=128, bias=True)\n",
    "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
    "  (fc3): Linear(in_features=64, out_features=4, bias=True)\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport agent, model, buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Implementation\n",
    "\n",
    "\n",
    "- Replay Buffer\n",
    "- Actor and Critic Network\n",
    "- OUNoise\n",
    "- Agent\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as I\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import agent\n",
    "from agent import MultiAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Replay Buffer\n",
    "\n",
    "Reinforcement learning is unstable when a nonlinear function approximator is used to represent action-value function, because the sequence of experiencs can be highly correlated. DDPG Agent stores the experience at each time step. Then by sampling from the buffer at random, It can prevent action values from oscillating or diverging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Actor and Critic Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the papar, The authors initialized the final layer weights and biases of both the actor and critic from a uniform distribution $ [−3×10^−3, 3×10^−3] $ and $[3×10^−4, 3×10^−4]$. This was to ensure the initial outputs for the policy and value estimates were near zero. The other layers were initialized from uniform distributions $ [− \\sqrt{f} , \\sqrt{f} ] $ where f is the fan-in of the layer.\n",
    "\n",
    "Since eveny entry in the action vector should be a number between -1 and 1, The activation function is **tanh**.\n",
    "Other hidden layers is activated by **relu**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.OUNoise\n",
    "\n",
    "To encourage agnet do exploration at initial step, add noise from Ornstein–Uhlenbeck noise process to the specific action produced by the actor(policy) network. The variation of the noise process decreases as time goes by. Therefore it can lead to reducing the exploration as the agent train. Also 2 consecutive samples are temporally correlated. This will ensure that 2 consecutive actions are not different widly. The authors use theta=0.15 sigma=0.2 for this noise process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Plot of Rewards\n",
    "---\n",
    "### 1.Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "# \n",
    "env = UnityEnvironment(file_name='data/multi_agent/Reacher_Windows_x86_64/Reacher.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "Shape of next state: (20, 33)\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "\n",
    "# shape of its elements\n",
    "print('Shape of next state: {}'.format(env_info.vector_observations.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_previous_model = True\n",
    "model_name = 'model_6_single'\n",
    "model_save_path = 'model/'\n",
    "params = {}\n",
    "# exp_0 specs\n",
    "params['BUFFER_SIZE'] = int(1e5)\n",
    "params['BATCH_SIZE'] = 256\n",
    "params['GAMMA'] = 0.99\n",
    "params['TAU'] = 1e-3\n",
    "params['LR_ACTOR'] = 1e-3\n",
    "params['LR_CRITIC'] = 1e-4\n",
    "params['WEIGHT_DECAY'] = 0.0001\n",
    "params['NOISE_SIGMA'] = 0.12\n",
    "params['TRAIN_ITER'] = 17\n",
    "params['TRAIN_FREQ'] = 10\n",
    "params['FC1'] = 128\n",
    "params['FC2'] = 96\n",
    "params['SEED'] = 7\n",
    "params['PRIORITY_SELECTION'] = False\n",
    "params['CLIPPING'] = False\n",
    "# TRUE_BATCH_SIZE = 128\n",
    "# EPSILON = 0.5\n",
    "\n",
    "\n",
    "# model save frequency\n",
    "CHECKPOINT_FREQ = 3\n",
    "# score printing frequency\n",
    "PRINT_EVERY = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up(model_name, params):\n",
    "    \"\"\"\n",
    "    set up for traning\n",
    "    If the model and its score already exist, load the model and score\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        \n",
    "    Returns\n",
    "    ======\n",
    "        scores (list) : score history \n",
    "        score_deque (deque) : score window \n",
    "        current_episode (int) : the episode number \n",
    "        multiAgent (MultiAgent)\n",
    "    \"\"\"\n",
    "    target_path = model_save_path+model_name\n",
    "    # 만약 해당 모델 네임으로된 폴더가 존재\n",
    "    if os.path.exists(target_path):\n",
    "        if not glob.glob(target_path+'/*.pth'):#os.exists(target_path+'/params.json'):\n",
    "            raise Exception('Previous model file doesnt exists')\n",
    "        # reset params for saved params.json\n",
    "        with open(target_path+'/params.json', 'r') as f:\n",
    "            # previous param dict\n",
    "            prev_params = json.load(f)\n",
    "        # load previous score data    \n",
    "        with open(target_path+'/scores.json', 'r') as f:\n",
    "            prev_score_data = json.load(f)\n",
    "            score_deque = deque(maxlen=100)\n",
    "            scores = []\n",
    "            current_episode = 1\n",
    "            if len(prev_score_data) !=0 :\n",
    "                current_episode = prev_score_data['current_episode']\n",
    "                scores = prev_score_data['scores']\n",
    "                for score in scores:\n",
    "                    score_deque.append(score)\n",
    "            \n",
    "        multiAgent = MultiAgent(state_size, action_size, num_agents, params)\n",
    "        multiAgent.load_model(path=target_path)\n",
    "        \n",
    "    # If this is first time ; no previous saved model\n",
    "    else:\n",
    "        print('This is first time to run the model : {}'.format(model_name))\n",
    "        os.makedirs(target_path)\n",
    "        # Write parameters into json file\n",
    "        with open(target_path+'/params.json', 'w') as f:\n",
    "            json.dump(params, f)\n",
    "        with open(target_path+'/scores.json', 'w') as f:\n",
    "            json.dump({}, f)\n",
    "            \n",
    "        current_episode = 1\n",
    "        scores = []\n",
    "        score_deque = deque(maxlen=100)\n",
    "        multiAgent = MultiAgent(state_size, action_size, num_agents, params)\n",
    "    \n",
    "    return multiAgent, scores, score_deque, current_episode, target_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiAgent, scores, score_deque, current_episode, target_path = set_up(model_name, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=250, timestep_max=1000):\n",
    "    # for plotting score grpha\n",
    "#     scores = []\n",
    "    # for calculating mean score of consecutive episodes.\n",
    "#     score_deque = deque(maxlen=100)\n",
    "    for i in range(current_episode,n_episodes+1):\n",
    "        # Get Initial State\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        # noise reset\n",
    "        multiAgent.reset()\n",
    "        # Episode score\n",
    "        score = np.zeros(num_agents) # shape : [num_agents,]\n",
    "        for t in range(timestep_max):\n",
    "            actions = multiAgent.act(states)\n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "        \n",
    "            multiAgent.step(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            states = next_states\n",
    "            score += rewards\n",
    "            # any agent arrives at terminal state.\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        \n",
    "        score_deque.append(np.mean(score))\n",
    "        scores.append(np.mean(score))\n",
    "        \n",
    "        print('\\rEpisode : {} \\t Current Score: {:.2f} \\t Average Score : {:.2f}'.format(i, np.mean(score), np.mean(score_deque)))\n",
    "        if i % CHECKPOINT_FREQ == 0:\n",
    "            multiAgent.save_model(target_path)\n",
    "            with open(target_path+'/scores.json', mode='w') as f:\n",
    "                score_data = {'current_episode':i, 'scores':scores}\n",
    "                json.dump(score_data, f)\n",
    "                \n",
    "        # print('\\rEpisode: {}\\t Score: {:.2f}'.format(i, score), end=\"\")\n",
    "    \n",
    "        if i% PRINT_EVERY == 0:\n",
    "            print('\\rEpisode : {} \\t Current Score: {:.2f} \\t Average Score : {:.2f}'.format(i, np.mean(score), np.mean(score_deque)))\n",
    "        if np.mean(score_deque)>=30:\n",
    "            print('The number of episodes needed to solve the problem : {}'.format(i))\n",
    "            # save model parameters\n",
    "            multiAgent.save_model(target_path)\n",
    "            break           \n",
    "            \n",
    "    return scores, multiAgent.actor_loss_history, multiAgent.critic_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores, actor_loss, critic_loss= ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(actor_loss+1), 1), actor_loss)\n",
    "plt.ylabel('average score')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(critic_loss+1), 1), critic_loss)\n",
    "plt.ylabel('average score')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hyper paramter를 조정해본다\n",
    "    \n",
    "    Authors get the optiaml hyper paratmeter by extensive trial to the task. and obviously the task in the paper and this project task are different. so there might be a better set of hyperparamters.\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
