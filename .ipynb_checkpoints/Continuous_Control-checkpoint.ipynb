{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "def lotto(chasu):\n",
    "    \n",
    "    url = \"https://dhlottery.co.kr/common.do?method=getLottoNumber&drwNo=\"+str(chasu)\n",
    "    result_data = urlopen(url)\n",
    "    result = result_data.read()\n",
    "    \n",
    "    data = json.loads(result)\n",
    "    \n",
    "    data_1 = pd.DataFrame.from_dict(data, orient='index')\n",
    "    data_1 = data_1.transpose()\n",
    "    \n",
    "    return data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lotto: 100%|██████████| 99/99 [00:13<00:00,  7.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "lotto_db = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(1,100), desc=\"lotto\", mininterval=1):\n",
    "    \n",
    "    data_1 = lotto(i)\n",
    "    lotto_db = pd.concat([lotto_db, data_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59240dc3cc904107921eacb60af8be7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=4, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4277e6141e944946921f5fae34695ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='2st loop', style=ProgressStyle(description_width='initial')),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0b281ed6f8469299e85b20a3ca8a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='2st loop', style=ProgressStyle(description_width='initial')),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a1a71ec33046aa92ab25f306bbf32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='2st loop', style=ProgressStyle(description_width='initial')),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a4ca631e304642a7d0424f31c22a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='2st loop', style=ProgressStyle(description_width='initial')),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tnrange\n",
    "from time import sleep\n",
    "\n",
    "for i in tnrange(4, desc='1st loop'):\n",
    "    for j in tnrange(100, desc='2st loop'):\n",
    "        sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_test(n):\n",
    "    for i in range(n):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005111342995169082\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "func_test(300000)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004957437515258789\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "func_test(300000)\n",
    "\n",
    "stop = time.time()\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'episode' : 0, 'currnet_score' : 10, 'avg_score':9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'episode' : i, 'current_score': i+10, 'avg_score': i+9} for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'episode': 0, 'current_score': 10, 'avg_score': 9},\n",
       " {'episode': 1, 'current_score': 11, 'avg_score': 10},\n",
       " {'episode': 2, 'current_score': 12, 'avg_score': 11},\n",
       " {'episode': 3, 'current_score': 13, 'avg_score': 12},\n",
       " {'episode': 4, 'current_score': 14, 'avg_score': 13},\n",
       " {'episode': 5, 'current_score': 15, 'avg_score': 14},\n",
       " {'episode': 6, 'current_score': 16, 'avg_score': 15},\n",
       " {'episode': 7, 'current_score': 17, 'avg_score': 16},\n",
       " {'episode': 8, 'current_score': 18, 'avg_score': 17},\n",
       " {'episode': 9, 'current_score': 19, 'avg_score': 18}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abc.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= open('test.json', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'['"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.json', 'r') as f:\n",
    "    prev_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_data[0]['episode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'episode': 0, 'current_score': 10, 'avg_score': 9},\n",
       " {'episode': 1, 'current_score': 11, 'avg_score': 10},\n",
       " {'episode': 2, 'current_score': 12, 'avg_score': 11},\n",
       " {'episode': 3, 'current_score': 13, 'avg_score': 12},\n",
       " {'episode': 4, 'current_score': 14, 'avg_score': 13},\n",
       " {'episode': 5, 'current_score': 15, 'avg_score': 14},\n",
       " {'episode': 6, 'current_score': 16, 'avg_score': 15},\n",
       " {'episode': 7, 'current_score': 17, 'avg_score': 16},\n",
       " {'episode': 8, 'current_score': 18, 'avg_score': 17},\n",
       " {'episode': 9, 'current_score': 19, 'avg_score': 18}]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-7dab8f4dcfef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprev_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "for i in prev_data.items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_list = [data['avg_score'] for data in prev_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode : 0 \t Current Score: 10.00 \t Average Score : 9.00\n",
      "\r",
      "Episode : 1 \t Current Score: 11.00 \t Average Score : 10.00\n",
      "\r",
      "Episode : 2 \t Current Score: 12.00 \t Average Score : 11.00\n",
      "\r",
      "Episode : 3 \t Current Score: 13.00 \t Average Score : 12.00\n",
      "\r",
      "Episode : 4 \t Current Score: 14.00 \t Average Score : 13.00\n",
      "\r",
      "Episode : 5 \t Current Score: 15.00 \t Average Score : 14.00\n",
      "\r",
      "Episode : 6 \t Current Score: 16.00 \t Average Score : 15.00\n",
      "\r",
      "Episode : 7 \t Current Score: 17.00 \t Average Score : 16.00\n",
      "\r",
      "Episode : 8 \t Current Score: 18.00 \t Average Score : 17.00\n",
      "\r",
      "Episode : 9 \t Current Score: 19.00 \t Average Score : 18.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(prev_data)):\n",
    "    print('\\rEpisode : {} \\t Current Score: {:.2f} \\t Average Score : {:.2f}'\\\n",
    "          .format(prev_data[i]['episode'], prev_data[i]['current_score'], prev_data[i]['avg_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.json', mode='a') as feed:\n",
    "    info = {'episode' : 100, 'currnet_score' : 10, 'avg_score':9}\n",
    "    feed.write(json.dumps(info))\n",
    "    feed.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.json', mode='r') as f:\n",
    "    d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_score = [1,2,3,4,5,6]\n",
    "avg_score = [6,5,4,3,2,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_previous_model = True\n",
    "prev_model_path = 'data/tmp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiAgent = MultiAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_print_prev_scores(prev_model_path):\n",
    "    \"\"\"\n",
    "    Load previous saved scores and print.\n",
    "    Get the current episode index and set up the scores list.\n",
    "    \"\"\"\n",
    "    # Load previouse score info\n",
    "    with open(prev_model_path+'prev_scores.json', 'r') as f:\n",
    "            prev_data = json.load(f)\n",
    "            \n",
    "    # Print prev scores\n",
    "    for i in range(len(prev_data)):\n",
    "        print('\\rEpisode : {} \\t Current Score: {:.2f} \\t Average Score : {:.2f}'\\\n",
    "              .format(prev_data[i]['episode'], prev_data[i]['current_score'], prev_data[i]['avg_score']))\n",
    "    \n",
    "    avg_scores = [data['avg_score'] for data in prev_data]\n",
    "    cur_episode = len(prev_data)+1\n",
    "    \n",
    "    return cur_episode, avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=100, timestep_max=1000, print_every=50, \\\n",
    "         load_previous_model=False, prev_model_path='data/tmp/', new_json_name='scores.json'):\n",
    "    # for plotting score grpha\n",
    "    avg_scores = []\n",
    "    # for calculating mean score of consecutive episodes.\n",
    "    score_deque = deque(maxlen=100)\n",
    "    \n",
    "    # If using previouse model\n",
    "    if load_previous_model:\n",
    "        # Load previouse score info\n",
    "        cur_episode, avg_scores = load_print_prev_scores(prev_model_path)\n",
    "        # Load previouse model paramters\n",
    "        multiAgent.load_model(prev_model_path)\n",
    "    else:\n",
    "        cur_episode = 1\n",
    "        \n",
    "    with open('data/tmp/'+new_json_name, mode='w') as f:\n",
    "        json.dump([], f)\n",
    "        \n",
    "    for i in range(cur_episode,n_episodes+1):\n",
    "        # Get Initial State\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        # noise reset\n",
    "        multiAgent.reset()\n",
    "        # Episode score\n",
    "        score = np.zeros(num_agents) # shape : [num_agents,]\n",
    "        for t in range(timestep_max):\n",
    "            actions = multiAgent.act(states)\n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "        \n",
    "            multiAgent.step(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            states = next_states\n",
    "            score += rewards\n",
    "            # any agent arrives at terminal state.\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        \n",
    "        single_score = np.mean(score)\n",
    "        score_deque.append(single_score)\n",
    "        scores.append(single_score)\n",
    "        \n",
    "        # print('\\rEpisode: {}\\t Score: {:.2f}'.format(i, score), end=\"\")\n",
    "        print('\\rEpisode : {} \\t Current Score: {:.2f} \\t Average Score : {:.2f}'\\\n",
    "              .format(i, single_score, np.mean(score_deque)))\n",
    "        \n",
    "        if i%print_every == 0:\n",
    "            print('\\rEpisode : {} \\t Current Score: {:.2f} \\t Average Score : {:.2f}'.format(i, single_score, np.mean(score_deque)))\n",
    "        \n",
    "        if np.mean(score_deque)>=30:\n",
    "            print('The number of episodes needed to solve the problem : {}'.format(i))\n",
    "            # save model parameters\n",
    "            multiAgent.save_model()\n",
    "            # save scores\n",
    "            with open('data/model/avg_scores.json', 'w') as f:\n",
    "                json.dump(avg_scores, f)\n",
    "            break \n",
    "        \n",
    "        # Save score info and model parameters\n",
    "        # Save score info\n",
    "        with open('data/tmp/'+new_json_name, mode='r') as f:\n",
    "            score_data = json.load(f)\n",
    "        current_data = {'episode' : i, 'currnet_score' : single_score, 'avg_score':np.mean(score_deque)}\n",
    "        score_data.append(current_data)\n",
    "        with open('data/tmp/'+new_json_name, mode='w') as f:\n",
    "            json.dump(score_data, f)\n",
    "\n",
    "        multiAgent.save_model()\n",
    "            \n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want save every score info and models\n",
    "    if save_freq==True:\n",
    "        if load_previous_model==False:\n",
    "            # Init the empty file\n",
    "            with open('data/tmp/prev_scores.json', mode='w') as f:\n",
    "                json.dump([], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [np.random.randn(4), np.random.randn(4), np.random.randn(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.36732462, -1.08478814, -0.7067744 , -1.62536732]),\n",
       " array([ 1.48424824,  0.28958292,  1.29872927, -0.23169664]),\n",
       " array([0.29292769, 0.46633514, 1.30348134, 1.54666591])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(l).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(1,3)\n",
    "b= np.arange(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.11002185,  0.36064328, -0.77506941]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.5988],\n",
       "        [-1.5551, -0.3414,  1.8530,  0.4681, -0.1577],\n",
       "        [ 1.4437,  0.2660,  1.3894,  1.5863,  0.9463],\n",
       "        [-0.8437,  0.9318,  1.2590,  2.0050,  0.0537]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.randn(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=[c for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.asarray(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =np.random.randn(1) < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "if a:\n",
    "    print(1)\n",
    "else:\n",
    "    print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x249c08817d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0845, -1.3986]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.],\n",
      "        [ 3.],\n",
      "        [ 4.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y = torch.tensor([[1.0],[2.0],[3.0]], requires_grad=True).to(device)\n",
    "    y = y+1\n",
    "\n",
    "print(y)\n",
    "q_val = torch.tensor([[0.5],[0.5],[0.5]], requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5000],\n",
      "        [ 0.5000],\n",
      "        [ 0.5000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(q_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5000],\n",
      "        [ 0.5000],\n",
      "        [ 0.5000]], device='cuda:0')\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "q_val_clone = q_val.detach().clone()\n",
    "print(q_val_clone)\n",
    "print(q_val_clone.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5000],\n",
      "        [ 2.5000],\n",
      "        [ 3.5000]], device='cuda:0')\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "td_error = y-q_val_clone\n",
    "print(td_error)\n",
    "print(td_error.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob=td_error/sum(td_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_numpy = prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2       ]\n",
      " [0.33333334]\n",
      " [0.46666667]]\n"
     ]
    }
   ],
   "source": [
    "print(prob_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.choice(3, 2, replace=False, p=prob_numpy.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5000],\n",
       "        [ 0.5000]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_val[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_val[index].shape\n",
    "y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ReacherBrain'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as I\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'brain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5a204aa478ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# size of each action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0maction_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_action_space_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Size of each action:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'brain' is not defined"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.local_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG\n",
    "\n",
    "---\n",
    "\n",
    "Lillicrap et al., CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING , https://arxiv.org/pdf/1509.02971.pdf\n",
    "\n",
    "\n",
    "**Summary**\n",
    "\n",
    "To make DQN angent deal with continuos action space, the authors suggest extension of actor-critic model called DDPG.\n",
    "The DDPG alogrithm is consists of Actor Critic Network, Replay Buffer, OUNoise(off-policy라서 독립적으로 exploration을 넣을 수 있어서)\n",
    "\n",
    "1. Main contribution : DDPG Algorithm\n",
    "2. others : soft update, usage of batch normalization, usage of OU noise\n",
    "\n",
    "\n",
    "Adam(1e-4 for actor, 1e-3 for critic)\n",
    "tau : 1e-3\n",
    "weight decay = 1e-2\n",
    "마지막 layer act : tanh\n",
    "\n",
    "Final layer weight and basis : uniform dist (-3x1e-3, 3x1e3) : to ensure the initial outputs for the policy and value estimates were near zero.\n",
    "\n",
    "Other layer : unifrom : 1 of root squareed f\n",
    "\n",
    "OU ; theta=0.15 sigma=0.2\n",
    "\n",
    "Actions were not\n",
    "included until the 2nd hidden layer of Q\n",
    "\n",
    "https://github.com/udacity/deep-reinforcement-learning/blob/master/ddpg-pendulum/ddpg_agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPG update 타이밍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Learning Alogorithm\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as I\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Replay Buffer\n",
    "\n",
    "Reinforcement learning is unstable when a nonlinear function approximator is used to represent action-value function, because the sequence of experiencs can be highly correlated. DDPG Agent stores the experience at each time step. Then by sampling from the buffer at random, It can prevent action values from oscillating or diverging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import random\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, buf_size, batch_size, seed):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ----------\n",
    "        buf_size (int): size of memory\n",
    "        batch_size (int): number of samples to be sampled\n",
    "        seed (int): random seed\n",
    "        \"\"\"\n",
    "        # When the replay buffer was full, the oldest sample needs to be discarded.\n",
    "        # So deque is suitalbe data structure. \n",
    "        self.memory = deque(maxlen=buf_size)\n",
    "        self.seed = random.seed(seed)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple('Trajectory', field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the size of memory\n",
    "        \"\"\"\n",
    "        return len(self.memory)\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add the agent's experiences at eacy time to the memory\n",
    "        \"\"\"\n",
    "        # Instantiate new experience with custom nemaedTuple\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        # Add the tuple to the memory\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Draw a sample.\n",
    "        Since the sample data is used by pytorch model, It needs to be converted to a torch Tensor.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        A tuple of torch tensor. Each tenosr's outermost dimension is batch_size.\n",
    "        \"\"\"\n",
    "        # list of sampled experience namedtuple of size of self.batch_size\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        # states : [batch_size, state.shape]\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        # sicne action is used by torch.gather(), It converts to long type.\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        # dones is needed to calculated the Q-value. At terminal state(dones=1), the Q-value should be just latest rewards.\n",
    "        # Convert it to np.unit8\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.unit8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Actor and Critic Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPG Agent uses 2 kinds of networks. Actor network is responsible for the policy function approximator. Critic Network is reponsible for Q-value function approximator. Critic gets states and actions as input and ouputs the action-value. The paper states that the actions were not included until the 2nd hidden layer of Q. So In this Implementatiom, actions are merged into the hidden layer between the 1st and 2nd one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the papar, The authors initialized the final layer weights and biases of both the actor and critic from a uniform distribution $ [−3×10^−3, 3×10^−3] $ and $[3×10^−4, 3×10^−4]$. This was to ensure the initial outputs for the policy and value estimates were near zero. The other layers were initialized from uniform distributions $ [− \\sqrt{f} , \\sqrt{f} ] $ where f is the fan-in of the layer.\n",
    "\n",
    "Since eveny entry in the action vector should be a number between -1 and 1, The activation function is **tanh**.\n",
    "Other hidden layers is activated by **relu**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy Network : state -> specific action (Not Probability distribution of Actions)\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=128):\n",
    "        \"\"\"\n",
    "        Initialization\n",
    "        \n",
    "        Params\n",
    "        -------\n",
    "        state_size : Vector Observation space size(per agent) : 33\n",
    "        action_size : Vector Action space size(per agent) : 4\n",
    "        seed : seed\n",
    "        fc1_units : first hidden layer \n",
    "        fc2_units : seccond hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=state_size, out_features=fc1_units)\n",
    "        self.fc2 = nn.Linear(in_features=fc1_units, out_features=fc2_units)\n",
    "        #self.fc3 = nn.Linear(in_features=self.hidden2, out_features=self.hidden1)\n",
    "        self.fc3 = nn.Linear(in_features=fc2_units, out_features=action_size)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def get_fan_in(self, layer):\n",
    "        \"\"\"\n",
    "        Get the fan-in in each layer.\n",
    "        \"\"\"\n",
    "        fan_in = 1/np.sqrt(layer.in_features)\n",
    "        return -fan_in, fan_in\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize weights and bais in each layer\n",
    "        \"\"\"\n",
    "        I.uniform_(self.fc1.weight, *self.get_fan_in(self.fc1))\n",
    "        I.uniform_(self.fc2.weight, *self.get_fan_in(self.fc2))\n",
    "        I.uniform_(self.fc3.weight, -3*1e-3, 3*1e+3)\n",
    "            \n",
    "    def forward(self, state):\n",
    "        \n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Every entry in action vector : [-1, 1]\n",
    "        actions = F.tanh(self.fc3(x))\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=128):\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        # hidden layer for state pathway\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc_merged = nn.Linear(fc1_units+action_size, fc2_units) \n",
    "        self.fc2 = nn.Linear(fc2_units, fc1_units)\n",
    "        self.fc3 = nn.Linear(fc1_units, action_size)\n",
    "        \n",
    "        # Initialize Weights and Biases\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        I.uniform_(self.fc1.weight, *self.get_fan_in(self.fc1))\n",
    "        I.uniform_(self.fc_merged.weight, *self.get_fan_in(self.fc_merged))\n",
    "        I.uniform_(self.fc2.weight, *self.get_fan_in(self.fc2))\n",
    "        I.uniform_(self.fc3.weight, -3*1e-3, 3*1e+3)\n",
    "        \n",
    "    def get_fan_in(self, layer):\n",
    "        fan_in = 1/np.sqrt(layer.in_features)\n",
    "        return -fan_in, fan_in\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        \n",
    "        x = F.relu(self.fc1(state))\n",
    "        # state : [batch_size, state_size], action : [batch_size, action_size]\n",
    "        # merged : [batch_size, state_size + action_size]\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        x = F.relu(self.fc_merged(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q = self.fc3(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.OUNoise\n",
    "\n",
    "To encourage agnet do exploration at initial step, add noise from Ornstein–Uhlenbeck noise process to the specific action produced by the actor(policy) network. The variation of the noise process decreases as time goes by. Therefore it can lead to reducing the exploration as the agent train. Also 2 consecutive samples are temporally correlated. This will ensure that 2 consecutive actions are not different widly.\n",
    "\n",
    "The authors use theta=0.15 sigma=0.2 for this noise process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"\n",
    "    Ornstein–Uhlenbeck noise\n",
    "    \"\"\"\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"\n",
    "        Intialization\n",
    "        \n",
    "        Params\n",
    "        ------\n",
    "        size : number of noise. It should be action_size = size\n",
    "        seed : random seed\n",
    "        mu : mean, defalut : 0 to reduce the noise over time.\n",
    "        theta : hyper paramter\n",
    "        sigam : hyper paramter\n",
    "        \"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the internal state to mu\n",
    "        \"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Process the internal state.\n",
    "        The Wiener process states are sampled by random.random()\n",
    "        \n",
    "        Returns\n",
    "        -----\n",
    "        self.state\n",
    "        \"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma*np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x+dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 1e-2     # L2 weight decay\n",
    "UP_FREQ = 4 \n",
    "class DDPGAgent():\n",
    "    \"\"\"\n",
    "    RL Agent whose actor and critic networks \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"\n",
    "        Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        -------\n",
    "        state_size : \n",
    "        \n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Actor networks\n",
    "        self.actor_local = Actor(state_size, action_size, seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        # Critic networks\n",
    "        self.critic_local = Critic(state_size, action_size, seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # OUNoise\n",
    "        self.noise = OUNoise(size=action_size, seed=self.seed)\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"\n",
    "        Returns actions for state based on current policy.\n",
    "        Params\n",
    "        -----\n",
    "        state : numpy array [1xstate_size]\n",
    "        add_noise : boolean , default: True\n",
    "        Returns\n",
    "        -----\n",
    "        action : numpy array ,shape:[1 x action_size]\n",
    "        \"\"\"\n",
    "        # Convert state into torch tensor\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        # Set evaluation mode\n",
    "        self.actor_local.eval()\n",
    "        # Disable gradient calculation\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        \n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "            \n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store experience in replay buffer\n",
    "        \"\"\"\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.memory)>BATCH_SIZE:\n",
    "            # Get minibatch experiences\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"\n",
    "        Update actor and critic network\n",
    "        \n",
    "        Params\n",
    "        -----\n",
    "        experiences (Tuple[torch.Tensor])\n",
    "        gamma (float)\n",
    "        \"\"\"\n",
    "        # Each of them is batch sized torch Tenosr\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # Update critic\n",
    "        # 1. Get next actions\n",
    "        next_actions = self.actor_target(next_states)\n",
    "        # 2. Get target value from the target network \n",
    "        y = rewards + gamma*self.critic_target(states, next_actions)*(1-dones)\n",
    "        # 3. Critic objective function\n",
    "        critic_loss = F.mse_loss(self.critic_local(states, actions), y)\n",
    "        # 4. Minimize\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Update actor\n",
    "        # 1. Get predicted action\n",
    "        pred_actions = self.actor_local(states)\n",
    "        # 2. Actor objective\n",
    "        actor_loss = -self.critic_local(states, pred_actions).mean()\n",
    "        # 3. Minimize loss\n",
    "        self.actor_optimizer.grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Update target networks\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        \n",
    "        \n",
    "    def soft_update(self, local, target, tau):\n",
    "        \"\"\"\n",
    "        Soft update\n",
    "        \n",
    "        𝜃_target = 𝜏*𝜃_local + (1 - 𝜏)*𝜃_target\n",
    "        \n",
    "        \"\"\"\n",
    "        # .parameters() is generator\n",
    "        for target_param, local_param in zip(target.parameters(), local.parameters()):\n",
    "            target_param.data.copy_(tau*local_param + (1-tau)*local_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Plot of Rewards\n",
    "---\n",
    "### 1.Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPGAgent(state_size=state_size, action_size=action_size, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=1000, timestep_max=2000, print_every=100):\n",
    "    scores = []\n",
    "    score_deque = deque(maxlen=print_every)\n",
    "    for i_episode in range(n_episodes):\n",
    "        # Get Initial State\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        # noise reset\n",
    "        agent.reset()\n",
    "        # Episode score\n",
    "        score = 0\n",
    "        for t in range(timestep_max):\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards\n",
    "            done = env_info.local_done\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        score_deque.append(score)\n",
    "        scores.append(score)\n",
    "        \n",
    "        # save model parameters\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        if i_episode%print_every == 0:\n",
    "            print('Episode: {}\\t Average score: {}'.format(i_episode, np.mean(score_deque)))\n",
    "            \n",
    "    return scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socres = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores+1), 1), socres)\n",
    "plt.ylabel('average score')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hyper paramter를 조정해본다\n",
    "    \n",
    "    Authors get the optiaml hyper paratmeter by extensive trial to the task. and obviously the task in the paper and this project task are different. so there might be a better set of hyperparamters.\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
